from pyspark.sql.functions import *
from pyspark.sql.types import *
  
Staging_1_df=spark.read.option("header", "true").csv("/FileStore/tables/Staging_1.csv")
# Staging_2_df=spark.read.option("header", "true").csv("/FileStore/tables/Staging_1.csv")
Staging_GMV_details_df=spark.read.option("header", "true").csv("/FileStore/tables/Staging_GMV_detail.csv")
Datanbase_loyal_df=spark.read.option("header", "true").csv("/FileStore/tables/Database_Loyal.csv")
Databse_NonLoyal_df=spark.read.option("header", "true").csv("/FileStore/tables/Databse_NonLoyal_df.csv")

Staging_1_df.display()
Staging_1_df.printSchema()
Staging_GMV_details_df.printSchema()
Datanbase_loyal_df.printSchema()
Databse_NonLoyal_df.printSchema()

#Staging_1_df.printSchema()
from pyspark.sql.functions import col


def func(df):
    df_renamed = df
    for old_name in df.columns:
        new_name = old_name.replace(" ", "_")
        if old_name != new_name:
            df_renamed = df_renamed.withColumnRenamed(old_name, new_name)
    return df_renamed

Staging_1_cleaned_df= func(Staging_1_df)
Staging_GMV_details_cleaned_df=func(Staging_GMV_details_df)
Datanbase_loyal_cleaned_df=func(Datanbase_loyal_df)
Databse_NonLoyal_cleaned_df=func(Databse_NonLoyal_df)

print(Staging_1_cleaned_df)

from delta.tables import *


Staging_1_cleaned_df.write.mode("overwrite").format("delta").saveAsTable("raw_catalog.bronze.Staging_table")
Staging_GMV_details_cleaned_df.write.mode("overwrite").format("delta").saveAsTable("raw_catalog.bronze.GMV_table")
Datanbase_loyal_cleaned_df.write.mode("overwrite").format("delta").saveAsTable("raw_catalog.bronze.Loyal_table")
Databse_NonLoyal_cleaned_df.write.mode("overwrite").format("delta").saveAsTable("raw_catalog.bronze.NonLoyal_table")

Staging_bronze_dataframe= spark.read.table("raw_catalog.bronze.Staging_table")
GMV_bronze_dataframe=spark.read.table("raw_catalog.bronze.GMV_table")
Loyal_bronze_dataframe=spark.read.table("raw_catalog.bronze.Loyal_table")
NonLoyal_bronze_dataframe=spark.read.table("raw_catalog.bronze.NonLoyal_table")

Staging_bronze_dataframe.display()
Staging_bronze_dataframe.printSchema()

from pyspark.sql.functions import col, to_date
Staging_bronze_dataframe = Staging_bronze_dataframe.withColumn("Date", to_date(col("Date"), "dd-MMM-yy"))

GMV_bronze_dataframe.display()
GMV_bronze_dataframe.printSchema()

from pyspark.sql.functions import col

GMV_bronze_dataframe = GMV_bronze_dataframe.withColumn("Revenue", col("Revenue").cast("double"))
GMV_bronze_dataframe = GMV_bronze_dataframe.withColumn("Tax", col("Tax").cast("double"))
GMV_bronze_dataframe = GMV_bronze_dataframe.withColumn("Bookings", col("Bookings").cast("int"))
GMV_bronze_dataframe = GMV_bronze_dataframe.withColumn("Quantity", col("Quantity").cast("int"))

Loyal_bronze_dataframe.display()

Loyal_bronze_dataframe=Loyal_bronze_dataframe.withColumn("txn_amount", col("txn_amount").cast("double"))\
                                            .withColumn("date", to_date(col("date"), "dd-MM-yyyy"))

NonLoyal_bronze_dataframe.display()
NonLoyal_bronze_dataframe.printSchema()

NonLoyal_bronze_dataframe.withColumn("txn_amount", col("txn_amount").cast("double"))\
                         .withColumn("date", to_date(col("date"), "dd-MM-yyyy"))

Staging_bronze_dataframe.printSchema()

Staging_bronze_dataframe.write.option("overwriteSchema", True).format("delta").mode("overwrite").saveAsTable("raw_catalog.silver.Staging_silver")
GMV_bronze_dataframe.write.format("delta").mode("overwrite").saveAsTable("raw_catalog.silver.GMV_silver")
Loyal_bronze_dataframe.write.format("delta").mode("overwrite").saveAsTable("raw_catalog.silver.Loyal_silver")
NonLoyal_bronze_dataframe.write.format("delta").mode("overwrite").saveAsTable("raw_catalog.silver.NonLoyal_silver")
